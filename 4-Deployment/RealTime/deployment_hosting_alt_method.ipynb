{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon SageMaker Workshop\n",
    "## _**Deployment**_\n",
    "\n",
    "---\n",
    "\n",
    "In this part of the workshop we will deploy our model created in the previous lab in an endpoint for real-time inferences to Predict Mobile Customer Departure.\n",
    "\n",
    "---\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Model hosting](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-hosting.html)\n",
    "  * Set up a persistent endpoint to get predictions from your model\n",
    " \n",
    "2. [Exercise - You turn to an endpoint and customize inference](#Exercise)\n",
    "  \n",
    "---\n",
    "\n",
    "## Background\n",
    "\n",
    "In the previous labs [Modeling](../../2-Modeling/modeling.ipynb) and [Evaluation](../../3-Evaluation/evaluation.ipynb) we trained multiple models with multiple SageMaker training jobs and evaluated them .\n",
    "\n",
    "In SageMaker, there are multiple methods to deploy a trained model to a Real-Time Inference endpoint: SageMaker SDK, AWS SDK - Boto3, and SageMaker console. For more information, see Deploy Models for Inference in the Amazon SageMaker Developer Guide. SageMaker SDK has more abstractions compared to the AWS SDK - Boto3, with the latter exposing lower-level APIs for greater control over model deployment. In this tutorial, you deploy the model using the AWS SDK -Boto3. There are three steps you need to follow in sequence to deploy a model:\n",
    "\n",
    "    1. Create a SageMaker model from the model artifact\n",
    "    2. Create an endpoint configuration to specify properties, including instance type and count\n",
    "    3. Create the endpoint using the endpoint configuration\n",
    "\n",
    "Let's import the libraries for this lab:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Supress default INFO loggingd\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "from time import strftime, gmtime\n",
    "\n",
    "import boto3\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.predictor import csv_serializer\n",
    "from sagemaker.model_monitor import DataCaptureConfig, DatasetFormat, DefaultModelMonitor\n",
    "from sagemaker.s3 import S3Uploader, S3Downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = boto3.Session()\n",
    "sm = sess.client('sagemaker')\n",
    "role = sagemaker.get_execution_role()\n",
    "s3_client = boto3.client(\"s3\", region_name=region)\n",
    "sm_client = boto3.client(\"sagemaker\", region_name=region)\n",
    "sm_runtime_client = boto3.client(\"sagemaker-runtime\")\n",
    "sm_autoscaling_client = boto3.client(\"application-autoscaling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r bucket\n",
    "%store -r prefix\n",
    "%store -r region\n",
    "%store -r docker_image_name\n",
    "%store -r framework_version\n",
    "%store -r s3uri_test\n",
    "%store -r training_job_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('sagemaker-studio-us-west-2-917049230680',\n",
       " 'xgboost-churn',\n",
       " 'us-west-2',\n",
       " '246618743249.dkr.ecr.us-west-2.amazonaws.com/sagemaker-xgboost:1.3-1',\n",
       " '1.3-1',\n",
       " 'workshop-framework-xgboost-customer-chu-2022-09-16-07-12-01-702',\n",
       " 's3://sagemaker-studio-us-west-2-917049230680/xgboost-churn/data/test/test.csv')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket, prefix, region, docker_image_name, framework_version,training_job_name,s3uri_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('s3://sagemaker-studio-us-west-2-917049230680/xgboost-churn/output/workshop-framework-xgboost-customer-chu-2022-09-16-07-12-01-702/output/model.tar.gz',\n",
       " 's3://sagemaker-studio-us-west-2-917049230680/xgboost-churn/datacapture',\n",
       " 's3://sagemaker-studio-us-west-2-917049230680/xgboost-churn/data/test/test.csv')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a model s3 path URI\n",
    "s3_model_uri = f's3://{bucket}/{prefix}/output/{training_job_name}/output/model.tar.gz'\n",
    "s3_model_uri\n",
    "\n",
    "#Craete Data capture URI \n",
    "# S3 path where data captured at endpoint will be stored\n",
    "data_capture_uri = f\"s3://{bucket}/{prefix}/datacapture\"\n",
    "\n",
    "# S3 location of test data\n",
    "test_data_uri = s3uri_test\n",
    "s3_model_uri, data_capture_uri,test_data_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Host the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've trained the model, let's deploy it to a hosted endpoint. To monitor the model after it's hosted and serving requests, we'll also add configurations to capture data that is being sent to the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with name xgboost-customer-chu-2022-09-16-07-12-01-702 already exists! Change model name to create new\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.image_uris import retrieve\n",
    "# Retrieve the SageMaker managed XGBoost image\n",
    "training_image = retrieve(framework=\"xgboost\", region=region, version=\"1.3-1\")\n",
    "\n",
    "# Specify a unique model name that does not exist (truncated model name to accomodate limit of characters allowed (64) for endpoint name)\n",
    "model_name = training_job_name[19:] \n",
    "primary_container = {\n",
    "                     \"Image\": docker_image_name,\n",
    "                     \"ModelDataUrl\": s3_model_uri\n",
    "                    }\n",
    "\n",
    "model_matches = sm_client.list_models(NameContains=model_name)[\"Models\"]\n",
    "if not model_matches:\n",
    "    model = sm_client.create_model(ModelName=model_name,\n",
    "                                   PrimaryContainer=primary_container,\n",
    "                                   ExecutionRoleArn=role)\n",
    "else:\n",
    "    print(f\"Model with name {model_name} already exists! Change model name to create new\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint config with name xgboost-customer-chu-2022-09-16-07-12-01-702-ep-config already exists! Change endpoint config name to create new\n"
     ]
    }
   ],
   "source": [
    "# Endpoint Config name\n",
    "endpoint_config_name = f\"{model_name}-ep-config\"\n",
    "\n",
    "# Endpoint config parameters\n",
    "production_variant_dict = {\n",
    "                           \"VariantName\": \"Alltraffic\",\n",
    "                           \"ModelName\": training_job_name,\n",
    "                           \"InitialInstanceCount\": 1,\n",
    "                           \"InstanceType\": \"ml.m5.xlarge\",\n",
    "                           \"InitialVariantWeight\": 1\n",
    "                          }\n",
    "\n",
    "# Data capture config parameters\n",
    "data_capture_config_dict = {\n",
    "                            \"EnableCapture\": True,\n",
    "                            \"InitialSamplingPercentage\": 100,\n",
    "                            \"DestinationS3Uri\": data_capture_uri,\n",
    "                            \"CaptureOptions\": [{\"CaptureMode\" : \"Input\"}, {\"CaptureMode\" : \"Output\"}]\n",
    "                           }\n",
    "\n",
    "\n",
    "# Create endpoint config if one with the same name does not exist\n",
    "endpoint_config_matches = sm_client.list_endpoint_configs(NameContains=endpoint_config_name)[\"EndpointConfigs\"]\n",
    "if not endpoint_config_matches:\n",
    "    endpoint_config_response = sm_client.create_endpoint_config(\n",
    "                                                                EndpointConfigName=endpoint_config_name,\n",
    "                                                                ProductionVariants=[production_variant_dict],\n",
    "                                                                DataCaptureConfig=data_capture_config_dict\n",
    "                                                               )\n",
    "else:\n",
    "    print(f\"Endpoint config with name {endpoint_config_name} already exists! Change endpoint config name to create new\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint with name xgboost-customer-chu-2022-09-16-07-12-01-702-ep already exists! Change endpoint name to create new\n",
      "Endpoint Status: InService\n"
     ]
    }
   ],
   "source": [
    "#code to create End point\n",
    "endpoint_name_ = f\"{model_name}-ep\"\n",
    "\n",
    "endpoint_matches = sm_client.list_endpoints(NameContains=endpoint_name)[\"Endpoints\"]\n",
    "if not endpoint_matches:\n",
    "    endpoint_response = sm_client.create_endpoint(\n",
    "                                                  EndpointName=endpoint_name,\n",
    "                                                  EndpointConfigName=endpoint_config_name\n",
    "                                                 )\n",
    "else:\n",
    "    print(f\"Endpoint with name {endpoint_name} already exists! Change endpoint name to create new\")\n",
    "\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "while status == \"Creating\":\n",
    "    print(f\"Endpoint Status: {status}...\")\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "print(f\"Endpoint Status: {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, we just trained a model with SageMaker and then used deployed it in a managed SageMaker endpoint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=https://us-east-2.console.aws.amazon.com/sagemaker/home?region=us-west-2#/endpoints>Look at your endpoints here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "sm_ep_placeholder = \"https://us-east-2.console.aws.amazon.com/sagemaker/home?region={}#/endpoints\"\n",
    "\n",
    "display(HTML(f\"<a href={sm_ep_placeholder.format(region)}>Look at your endpoints here</a>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or go to the left tab here, inside the Studio UI, and select \"Endpoints\":\n",
    "\n",
    "![endpoints.png](media/endpoints.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's save the endpoint name for later (Monitoring lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'endpoint_name_v2' (str)\n"
     ]
    }
   ],
   "source": [
    "endpoint_name_v2 = endpoint_name\n",
    "%store endpoint_name_v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke the inference endpoint\n",
    "\n",
    "Now that we have a hosted endpoint running, we can make real-time predictions from our model by making an http POST request.  But first, we need to set up serializers and deserializers for passing our `test_data` NumPy arrays to the model behind the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.009803025051951408</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.006166679784655571</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.3982780873775482</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.022325119003653526</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.026182275265455246</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Prediction  Label\n",
       "0  0.009803025051951408      0\n",
       "1  0.006166679784655571      0\n",
       "2    0.3982780873775482      0\n",
       "3  0.022325119003653526      0\n",
       "4  0.026182275265455246      0"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fetch test data to run predictions with the endpoint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3\n",
    "import sagemaker\n",
    "import time\n",
    "import json\n",
    "import io\n",
    "from io import StringIO\n",
    "import base64\n",
    "import pprint\n",
    "import re\n",
    "test_df = pd.read_csv(test_data_uri)\n",
    "# test_df.head(5)\n",
    "# For content type text/csv, payload should be a string with commas separating the values for each feature\n",
    "# This is the inference request serialization step\n",
    "# CSV serialization\n",
    "csv_file = io.StringIO()\n",
    "test_sample = test_df.drop(test_df.columns[0], axis=1).iloc[:5]\n",
    "test_sample.to_csv(csv_file, sep=\",\", header=False, index=False)\n",
    "payload = csv_file.getvalue()\n",
    "response = sm_runtime_client.invoke_endpoint(\n",
    "                                             EndpointName=endpoint_name,\n",
    "                                             Body=payload,\n",
    "                                             ContentType=\"text/csv\",\n",
    "                                             Accept=\"text/csv\"\n",
    "                                            )\n",
    "\n",
    "# This is the inference response deserialization step\n",
    "# This is a bytes object\n",
    "result = response[\"Body\"].read()\n",
    "# Decoding bytes to a string\n",
    "result = result.decode(\"utf-8\")\n",
    "# Converting to list of predictions\n",
    "result = re.split(\",|\\n\",result)\n",
    "\n",
    "prediction_df = pd.DataFrame()\n",
    "prediction_df[\"Prediction\"] = result[:5]\n",
    "prediction_df[\"Label\"] = test_df[test_df.columns[0]].iloc[:5].values\n",
    "prediction_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because data capture was set up in the endpoint configuration, you have a way to inspect what payload was sent to the endpoint alongside its response. The captured data takes some time to get fully uploaded to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for captures to show up..........................................................................................\n",
      "Found 3 Data Capture Files:\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.s3 import S3Downloader\n",
    "print(\"Waiting for captures to show up\", end=\"\")\n",
    "for _ in range(90):\n",
    "    capture_files = sorted(S3Downloader.list(f\"{data_capture_uri}/{endpoint_name}\"))\n",
    "    if capture_files:\n",
    "        capture_file = S3Downloader.read_file(capture_files[-1]).split(\"\\n\")\n",
    "        capture_record = json.loads(capture_file[0])\n",
    "        if \"inferenceId\" in capture_record[\"eventMetadata\"]:\n",
    "            break\n",
    "    print(\".\", end=\"\", flush=True)\n",
    "    time.sleep(1)\n",
    "print()\n",
    "print(f\"Found {len(capture_files)} Data Capture Files:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify that data is captured in Amazon S3\n",
    "\n",
    "When we made some real-time predictions by sending data to our endpoint, we should have also captured that data for monitoring purposes. \n",
    "\n",
    "Let's list the data capture files stored in Amazon S3. Expect to see different files from different time periods organized based on the hour in which the invocation occurred. The format of the Amazon S3 path is:\n",
    "\n",
    "`s3://{destination-bucket-prefix}/{endpoint-name}/{variant-name}/yyyy/mm/dd/hh/filename.jsonl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found Data Capture Files:\n",
      "['s3://sagemaker-studio-us-west-2-917049230680/xgboost-churn/datacapture/xgboost-customer-chu-2022-09-16-07-12-01-702-ep/Alltraffic/2022/10/07/04/03-10-985-618dc692-6f89-40b7-b295-050a8d25b8a7.jsonl', 's3://sagemaker-studio-us-west-2-917049230680/xgboost-churn/datacapture/xgboost-customer-chu-2022-09-16-07-12-01-702-ep/Alltraffic/2022/10/07/04/04-21-279-568647ac-7405-453e-901a-5947ddd9866b.jsonl', 's3://sagemaker-studio-us-west-2-917049230680/xgboost-churn/datacapture/xgboost-customer-chu-2022-09-16-07-12-01-702-ep/Alltraffic/2022/10/08/05/03-30-619-da332f99-d0cf-4654-aa38-ff7ba72d4459.jsonl']\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "\n",
    "current_endpoint_capture_prefix = '{}/{}'.format(data_capture_prefix, endpoint_name)\n",
    "for _ in range(12): # wait up to a minute to see captures in S3\n",
    "    capture_files = S3Downloader.list(\"s3://{}/{}\".format(bucket, current_endpoint_capture_prefix))\n",
    "    if capture_files:\n",
    "        break\n",
    "    sleep(5)\n",
    "\n",
    "print(\"Found Data Capture Files:\")\n",
    "print(capture_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the data captured is stored in a SageMaker specific json-line formatted file. Next, Let's take a quick peek at the contents of a single line in a pretty formatted json so that we can observe the format a little better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====Single Data Capture====\n",
      "{\n",
      "  \"captureData\": {\n",
      "    \"endpointInput\": {\n",
      "      \"observedContentType\": \"text/csv\",\n",
      "      \"mode\": \"INPUT\",\n",
      "      \"data\": \"MTMyLDI1LDExMy4yLDk2LDI2OS45LDEwNywyMjkuMSw4Nyw3LjEsNywyLDAsMCwwLDAsMCwwLDAsMCwwLDAsMCwwLDAsMCwwLDAsMCwwLDAsMSwwLDAsMCwwLDAsMCwwLDAsMCwwLDAsMCwwLDAsMCwwLDAsMCwwLDAsMCwwLDAsMCwwLDAsMCwwLDAsMCwwLDAsMSwwLDEsMCwwLDEKMTEyLDE3LDE4My4yLDk1LDI1Mi44LDEyNSwxNTYuNyw5NSw5LjcsMywwLDAsMCwwLDAsMCwwLDAsMCwwLDAsMCwwLDAsMCwwLDAsMCwwLDAsMSwwLDAsMCwwLDAsMCwwLDAsMCwwLDAsMCwwLDAsMCwwLDAsMCwwLDAsMCwwLDAsMCwwLDAsMCwwLDAsMCwwLDAsMSwwLDEsMCwwLDEKOTEsMjQsOTMuNSwxMTIsMTgzLjQsMTI4LDI0MC43LDEzMyw5LjksMywwLDAsMCwwLDAsMCwwLDAsMCwwLDEsMCwwLDAsMCwwLDAsMCwwLDAsMCwwLDAsMCwwLDAsMCwwLDAsMCwwLDAsMCwwLDAsMCwwLDAsMCwwLDAsMCwwLDAsMCwwLDAsMCwwLDAsMCwwLDAsMCwxLDAsMSwwLDEKMjIsMCwxMTAuMywxMDcsMTY2LjUsOTMsMjAyLjMsOTYsOS41LDUsMCwwLDAsMCwwLDAsMCwwLDAsMCwwLDAsMCwwLDAsMCwwLDAsMCwwLDAsMCwwLDAsMCwwLDAsMCwwLDAsMCwwLDAsMCwwLDAsMCwwLDAsMCwwLDEsMCwwLDAsMCwwLDAsMCwwLDAsMCwxLDAsMCwxLDAsMSwwCjEwMiwwLDE4Ni44LDkyLDE3My43LDEyMywyNTAuOSwxMzEsOS43LDQsMiwwLDAsMCwwLDAsMCwwLDEsMCwwLDAsMCwwLDAsMCwwLDAsMCwwLDAsMCwwLDAsMCwwLDAsMCwwLDAsMCwwLDAsMCwwLDAsMCwwLDAsMCwwLDAsMCwwLDAsMCwwLDAsMCwwLDAsMCwwLDEsMCwxLDAsMSwwCg==\",\n",
      "      \"encoding\": \"BASE64\"\n",
      "    },\n",
      "    \"endpointOutput\": {\n",
      "      \"observedContentType\": \"text/csv; charset=utf-8\",\n",
      "      \"mode\": \"OUTPUT\",\n",
      "      \"data\": \"MC4wMDk4MDMwMjUwNTE5NTE0MDgKMC4wMDYxNjY2Nzk3ODQ2NTU1NzEKMC4zOTgyNzgwODczNzc1NDgyCjAuMDIyMzI1MTE5MDAzNjUzNTI2CjAuMDI2MTgyMjc1MjY1NDU1MjQ2Cg==\",\n",
      "      \"encoding\": \"BASE64\"\n",
      "    }\n",
      "  },\n",
      "  \"eventMetadata\": {\n",
      "    \"eventId\": \"5b93330f-680f-407e-bf61-65c05810edb0\",\n",
      "    \"inferenceTime\": \"2022-10-08T05:03:30Z\"\n",
      "  },\n",
      "  \"eventVersion\": \"0\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "capture_file = S3Downloader.read_file(capture_files[-1])\n",
    "\n",
    "print(\"=====Single Data Capture====\")\n",
    "print(json.dumps(json.loads(capture_file.split('\\n')[0]), indent=2)[:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, each inference request is captured in one line in the jsonl file. The line contains both the input and output merged together. In our example, we provided the ContentType as `text/csv` which is reflected in the `observedContentType` value. Also, we expose the enconding that we used to encode the input and output payloads in the capture format with the `encoding` value.\n",
    "\n",
    "To recap, we have observed how you can enable capturing the input and/or output payloads to an Endpoint with a new parameter. We have also observed how the captured format looks like in S3. Let's continue to explore how SageMaker helps with monitoring the data collected in S3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## _Alternative deployment_\n",
    "\n",
    "Ok, nice! We can train with SageMaker and then deploy in a managed endpoint with monitoring enabled.\n",
    "\n",
    "But:\n",
    "\n",
    "#### - What if I already have a model that was trained outside of SageMaker? How do I deploy it in SageMaker without training it previously?\n",
    "\n",
    "#### - What if I need to preprocess the request before performing inference and then post process what my model just predicted. How can I customize the inference logic with a custom inference script?\n",
    "\n",
    "# Exercise\n",
    "### _[Challenge] Your turn!_\n",
    "\n",
    "Deploy another model in SageMaker. Remember that the output of each training job was an artifact (tar.gz file with the model and other configurations) that was saved in S3.\n",
    "\n",
    "1. Pick one of this models in S3 or upload another one from your laptop to S3. Then deploy it.\n",
    "(If you haven't trained a model, pick the `model.tar.gz` in the `config` directory).\n",
    "\n",
    "2. Add a custom inference script in your endpoint\n",
    "\n",
    "To make things easiser, you can add a simple post-processing function add a new value to the output `\"hello from post-processing function!!!` to the request.\n",
    "\n",
    "So, if we send to our endpoint: \n",
    "```\n",
    "186,0.1,137.8,97,187.7,118,146.4,85,8.7,6,1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.10,0.11,0.12,0.13,0.14,0.15,0.16,0.17,1.1,0.18,0.19,0.20,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.30,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.40,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.50,0.51,0.52,0.53,1.2,1.3,0.54,1.4,0.55\n",
    "``` \n",
    "\n",
    "The output will be something like:\n",
    "```\n",
    "0.014719205908477306,\"hello from post-processing\"\n",
    "```\n",
    "\n",
    "Want a hint? [Look here](./solutions/b-hint1.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# [You can now go to the lab 5-Monitoring](../../5-Monitoring/monitoring.ipynb)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
